{"config":{"lang":["en"],"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Data Science for Mathematicians \u00b6 Online resources supplementing the book The book Data Science for Mathematicians is to appear in 2020 from Taylor and Francis . Book homepage on T&F site Pre-order page on Amazon Table of Contents \u00b6 Foreward Author: Michael Pearson Why is this book important? See the Forward. Chapter 1: Introduction \u00b6 Author: Nathan Carter (book editor) Digital resources for Chapter 1 Chapter 2: Programming with Data \u00b6 Author: Sean Raleigh Digital resources for Chapter 2 Chapter 3: Linear Algebra \u00b6 Author: Jeffery Leader Digital resources for Chapter 3 Chapter 4: Basic Statistics \u00b6 Author: David White Digital resources for Chapter 4 Chapter 5: Clustering \u00b6 Author: Amy Wagaman Digital resources for Chapter 5 Chapter 6: Operations Research \u00b6 Authors: Alice Paul and Susan Martonosi Digital resources for Chapter 6 Chapter 7: Dimensionality Reduction \u00b6 Authors: Sofya Chepushtanova , Elin Farnell , Eric Kehoe , Michael Kirby , and Henry Kvinge Digital resources for Chapter 7 Chapter 8: Machine Learning \u00b6 Authors: Mahesh Agarwal , Nathan Carter , and David Oury Digital resources for Chapter 8 Chapter 9: Deep Learning \u00b6 Author: Samuel S. Watson Digital resources for Chapter 9 Chapter 10: Topological Data Analysis \u00b6 Authors: Henry Adams , Johnathan Bush , and Joshua Mirth Digital resources for Chapter 10","title":"Home"},{"location":"#data-science-for-mathematicians","text":"Online resources supplementing the book The book Data Science for Mathematicians is to appear in 2020 from Taylor and Francis . Book homepage on T&F site Pre-order page on Amazon","title":"Data Science for Mathematicians"},{"location":"#table-of-contents","text":"Foreward Author: Michael Pearson Why is this book important? See the Forward.","title":"Table of Contents"},{"location":"#chapter-1-introduction","text":"Author: Nathan Carter (book editor) Digital resources for Chapter 1","title":"Chapter 1: Introduction"},{"location":"#chapter-2-programming-with-data","text":"Author: Sean Raleigh Digital resources for Chapter 2","title":"Chapter 2: Programming with Data"},{"location":"#chapter-3-linear-algebra","text":"Author: Jeffery Leader Digital resources for Chapter 3","title":"Chapter 3: Linear Algebra"},{"location":"#chapter-4-basic-statistics","text":"Author: David White Digital resources for Chapter 4","title":"Chapter 4: Basic Statistics"},{"location":"#chapter-5-clustering","text":"Author: Amy Wagaman Digital resources for Chapter 5","title":"Chapter 5: Clustering"},{"location":"#chapter-6-operations-research","text":"Authors: Alice Paul and Susan Martonosi Digital resources for Chapter 6","title":"Chapter 6: Operations Research"},{"location":"#chapter-7-dimensionality-reduction","text":"Authors: Sofya Chepushtanova , Elin Farnell , Eric Kehoe , Michael Kirby , and Henry Kvinge Digital resources for Chapter 7","title":"Chapter 7: Dimensionality Reduction"},{"location":"#chapter-8-machine-learning","text":"Authors: Mahesh Agarwal , Nathan Carter , and David Oury Digital resources for Chapter 8","title":"Chapter 8: Machine Learning"},{"location":"#chapter-9-deep-learning","text":"Author: Samuel S. Watson Digital resources for Chapter 9","title":"Chapter 9: Deep Learning"},{"location":"#chapter-10-topological-data-analysis","text":"Authors: Henry Adams , Johnathan Bush , and Joshua Mirth Digital resources for Chapter 10","title":"Chapter 10: Topological Data Analysis"},{"location":"chapter-1-resources/","text":"Chapter 1: Introduction \u00b6 Author: Nathan Carter (book editor) Digital resources supplementing the chapter Blog post cited in the Chapter \u00b6 The Data Science Venn Diagram , Drew Conway, 3/26/2013 Data science professionals mentioned in the chapter \u00b6 Cathy O'Neil , mathematician, author, independent data science consultant Eric Place , Director, Data & Analytics, Martin's Point Health Care Bill Howe , Associate Director, eScience Institute, University of Washington Bill Franks , Chief Analytics Officer, International Institute for Analytics Rob Gould , Director, Center for Teaching Statistics, UCLA Jennifer Priestley , Professor of Statistics and Data Science, Associate Dean of the Graduate College, Kennesaw State University","title":"Chapter 1, Introduction"},{"location":"chapter-1-resources/#chapter-1-introduction","text":"Author: Nathan Carter (book editor) Digital resources supplementing the chapter","title":"Chapter 1: Introduction"},{"location":"chapter-1-resources/#blog-post-cited-in-the-chapter","text":"The Data Science Venn Diagram , Drew Conway, 3/26/2013","title":"Blog post cited in the Chapter"},{"location":"chapter-1-resources/#data-science-professionals-mentioned-in-the-chapter","text":"Cathy O'Neil , mathematician, author, independent data science consultant Eric Place , Director, Data & Analytics, Martin's Point Health Care Bill Howe , Associate Director, eScience Institute, University of Washington Bill Franks , Chief Analytics Officer, International Institute for Analytics Rob Gould , Director, Center for Teaching Statistics, UCLA Jennifer Priestley , Professor of Statistics and Data Science, Associate Dean of the Graduate College, Kennesaw State University","title":"Data science professionals mentioned in the chapter"},{"location":"chapter-10-resources/","text":"Chapter 10: Topological Data Analysis \u00b6 Authors: Henry Adams , Johnathan Bush , and Joshua Mirth Digital resources supplementing the chapter Persistent homology tutorial (4 parts) \u00b6 Computing persistent homology online Exercises Computing persistent homology on your machine Topological feature vectors (and bibliography) Persistent homology code \u00b6 This repository contains scripts for using Ripser and Hera with a Python interface. Code was written by Melissa McGuirl . You can run the scripts using Python on the command line, or inspect the code within the scripts to see how they are implemented. To run Ripser on a collection of distance matrices, run the script getBarCodes.py . To separate Ripser output by dimension to input that is compatible with Hera, run the script separateRipser.py . IMPORTANT: That script assumes there are just dimension 0 and dimension 1 barcodes, but it could be adapted easily. To plot persistence diagrams, run the script plotpd.py . (This script requires the output of separateRipser.py , not the output of getBarCodes.py .) To compute distances between two barcodes, run the script computePDDists.py . For help on using any of these scripts, run it with a --h switch, as in python getBarCodes.py --h . NOTE: The plan of the chapter authors is to update this code to instead use scikit-tda .","title":"Chapter 10, Topological Data Analysis"},{"location":"chapter-10-resources/#chapter-10-topological-data-analysis","text":"Authors: Henry Adams , Johnathan Bush , and Joshua Mirth Digital resources supplementing the chapter","title":"Chapter 10: Topological Data Analysis"},{"location":"chapter-10-resources/#persistent-homology-tutorial-4-parts","text":"Computing persistent homology online Exercises Computing persistent homology on your machine Topological feature vectors (and bibliography)","title":"Persistent homology tutorial (4 parts)"},{"location":"chapter-10-resources/#persistent-homology-code","text":"This repository contains scripts for using Ripser and Hera with a Python interface. Code was written by Melissa McGuirl . You can run the scripts using Python on the command line, or inspect the code within the scripts to see how they are implemented. To run Ripser on a collection of distance matrices, run the script getBarCodes.py . To separate Ripser output by dimension to input that is compatible with Hera, run the script separateRipser.py . IMPORTANT: That script assumes there are just dimension 0 and dimension 1 barcodes, but it could be adapted easily. To plot persistence diagrams, run the script plotpd.py . (This script requires the output of separateRipser.py , not the output of getBarCodes.py .) To compute distances between two barcodes, run the script computePDDists.py . For help on using any of these scripts, run it with a --h switch, as in python getBarCodes.py --h . NOTE: The plan of the chapter authors is to update this code to instead use scikit-tda .","title":"Persistent homology code"},{"location":"chapter-2-resources/","text":"Chapter 2: Programming with Data \u00b6 Author: Sean Raleigh Digital resources supplementing the chapter Markdown example \u00b6 Figure 2.1 of the text shows example uses of Markdown in a mathematical document. You can view that document in several ways here. As a Jupyter notebook Rendered to PDF from Jupyter As an RMarkdown file Knit to PDF from R Notebook examples \u00b6 Figures 2.2 through 2.4 of the text show example notebooks. You can view those notebooks online here. RMarkdown notebook from Figures 2.2 and 2.3 That notebook rendered to PDF Jupyter notebook from Figure 2.4 That notebook rendered to PDF Code examples \u00b6 Section 2.3 shows many small snippets of code while teaching coding best practices. Some are written in R and some in Python. The documents below contain those code snippets together with paragraphs from the text that explain them. RMarkdown document with R code from Section 2.3 In order to function, that document needs this R code file in the same folder. That same document rendered to a PDF Jupyter notebook with Python code from Section 2.3 That same notebook rendered to a PDF","title":"Chapter 2, Programming with Data"},{"location":"chapter-2-resources/#chapter-2-programming-with-data","text":"Author: Sean Raleigh Digital resources supplementing the chapter","title":"Chapter 2: Programming with Data"},{"location":"chapter-2-resources/#markdown-example","text":"Figure 2.1 of the text shows example uses of Markdown in a mathematical document. You can view that document in several ways here. As a Jupyter notebook Rendered to PDF from Jupyter As an RMarkdown file Knit to PDF from R","title":"Markdown example"},{"location":"chapter-2-resources/#notebook-examples","text":"Figures 2.2 through 2.4 of the text show example notebooks. You can view those notebooks online here. RMarkdown notebook from Figures 2.2 and 2.3 That notebook rendered to PDF Jupyter notebook from Figure 2.4 That notebook rendered to PDF","title":"Notebook examples"},{"location":"chapter-2-resources/#code-examples","text":"Section 2.3 shows many small snippets of code while teaching coding best practices. Some are written in R and some in Python. The documents below contain those code snippets together with paragraphs from the text that explain them. RMarkdown document with R code from Section 2.3 In order to function, that document needs this R code file in the same folder. That same document rendered to a PDF Jupyter notebook with Python code from Section 2.3 That same notebook rendered to a PDF","title":"Code examples"},{"location":"chapter-3-resources/","text":"Chapter 3: Linear Algebra \u00b6 Author: Jeffery Leader Digital resources supplementing the chapter None here yet; this page is a placeholder. Check back later.","title":"Chapter 3, Linear Algebra"},{"location":"chapter-3-resources/#chapter-3-linear-algebra","text":"Author: Jeffery Leader Digital resources supplementing the chapter None here yet; this page is a placeholder. Check back later.","title":"Chapter 3: Linear Algebra"},{"location":"chapter-4-resources/","text":"Chapter 4: Basic Statistics \u00b6 Author: David White Digital resources supplementing the chapter None here yet; this page is a placeholder. Check back later.","title":"Chapter 4, Basic Statistics"},{"location":"chapter-4-resources/#chapter-4-basic-statistics","text":"Author: David White Digital resources supplementing the chapter None here yet; this page is a placeholder. Check back later.","title":"Chapter 4: Basic Statistics"},{"location":"chapter-5-resources/","text":"Chapter 5: Clustering \u00b6 Author: Amy Wagaman Digital resources supplementing the chapter Much of the R code shown in the chapter is available for viewing and downloading from this repository, as well as the necessary related data files. Small clustering example \u00b6 View R Markdown file View generated PDF Iris data visualization \u00b6 View R Markdown file View generated PDF Wine data, k -means clustering \u00b6 View R Markdown file To run that R Markdown file on your own machine, you would need the associated data file in the same folder as the .Rmd file. You can get the data file here. View generated PDF Wine data, hierarchical clustering \u00b6 View R Markdown file To run that R Markdown file on your own machine, you would need the associated data file in the same folder as the .Rmd file. You can get the data file here. View generated PDF Wine data, model-based clustering \u00b6 View R Markdown file To run that R Markdown file on your own machine, you would need the associated data file in the same folder as the .Rmd file. You can get the data file here. You will also need a file containing BIC values, which you should also place in the same folder as the .Rmd file. You can get that second file here. View generated PDF Wine data, density-based clustering \u00b6 View R Markdown file To run that R Markdown file on your own machine, you would need the associated data file in the same folder as the .Rmd file. You can get the data file here. View generated PDF Clustering on dolphin network data \u00b6 View R Markdown file View generated PDF To run that R Markdown file on your own machine, you would need the associated data file in the same folder as the .Rmd file. You can get the data file here. k -means clustering with bad initial centroids \u00b6 View R Markdown file View generated PDF College Scorecard, two types of clustering compared \u00b6 View R Markdown file To run that R Markdown file on your own machine, you would need the associated data file in the same folder as the .Rmd file. You can get the data file here. View generated PDF","title":"Chapter 5, Clustering"},{"location":"chapter-5-resources/#chapter-5-clustering","text":"Author: Amy Wagaman Digital resources supplementing the chapter Much of the R code shown in the chapter is available for viewing and downloading from this repository, as well as the necessary related data files.","title":"Chapter 5: Clustering"},{"location":"chapter-5-resources/#small-clustering-example","text":"View R Markdown file View generated PDF","title":"Small clustering example"},{"location":"chapter-5-resources/#iris-data-visualization","text":"View R Markdown file View generated PDF","title":"Iris data visualization"},{"location":"chapter-5-resources/#wine-data-k-means-clustering","text":"View R Markdown file To run that R Markdown file on your own machine, you would need the associated data file in the same folder as the .Rmd file. You can get the data file here. View generated PDF","title":"Wine data, k-means clustering"},{"location":"chapter-5-resources/#wine-data-hierarchical-clustering","text":"View R Markdown file To run that R Markdown file on your own machine, you would need the associated data file in the same folder as the .Rmd file. You can get the data file here. View generated PDF","title":"Wine data, hierarchical clustering"},{"location":"chapter-5-resources/#wine-data-model-based-clustering","text":"View R Markdown file To run that R Markdown file on your own machine, you would need the associated data file in the same folder as the .Rmd file. You can get the data file here. You will also need a file containing BIC values, which you should also place in the same folder as the .Rmd file. You can get that second file here. View generated PDF","title":"Wine data, model-based clustering"},{"location":"chapter-5-resources/#wine-data-density-based-clustering","text":"View R Markdown file To run that R Markdown file on your own machine, you would need the associated data file in the same folder as the .Rmd file. You can get the data file here. View generated PDF","title":"Wine data, density-based clustering"},{"location":"chapter-5-resources/#clustering-on-dolphin-network-data","text":"View R Markdown file View generated PDF To run that R Markdown file on your own machine, you would need the associated data file in the same folder as the .Rmd file. You can get the data file here.","title":"Clustering on dolphin network data"},{"location":"chapter-5-resources/#k-means-clustering-with-bad-initial-centroids","text":"View R Markdown file View generated PDF","title":"k-means clustering with bad initial centroids"},{"location":"chapter-5-resources/#college-scorecard-two-types-of-clustering-compared","text":"View R Markdown file To run that R Markdown file on your own machine, you would need the associated data file in the same folder as the .Rmd file. You can get the data file here. View generated PDF","title":"College Scorecard, two types of clustering compared"},{"location":"chapter-6-resources/","text":"Chapter 6: Operations Research \u00b6 Authors: Alice Paul and Susan Martonosi Digital resources supplementing the chapter Sample R code \u00b6 R code for generating Figure 6.7 of the text Figure 6.7 shows two images: 1. a histogram of 1000 randomly generated U(0,1) U(0,1) values overlaid with the density of that distribution 2. a histogram of X=-\\frac{\\ln(1-U)}{\\lambda} X=-\\frac{\\ln(1-U)}{\\lambda} with \\lambda=5 \\lambda=5 created from the first distribution, with the exponential distribution (with \\lambda=5 \\lambda=5 ) overlaid R code for generating Table 6.1 of the text Table 6.1 illustrates several different probability distributions, including * discrete uniform, * binomial, * geometric, * Poisson, * uniform, * triangle, * normal, and * exponential. Each of those distributions can be accessed using code in a variety of languages, as shown below. The reader may also be interested in the individual documentation for probability distributions in R , MATLAB Python , or Julia . Code reference for random number generation \u00b6 In each example below, n n is the number of values to generate. All Python code assumes you have installed SciPy and have imported its statistics package with import scipy.stats as stats . All Julia code assumes you have installed Distributions.jl and have imported it with using Distributions . Discrete uniform distribution on \\{a,a+1,\\ldots,b\\} \\{a,a+1,\\ldots,b\\} \u00b6 In R: sample ( a : b , n , replace = TRUE ) In MATLAB: randi ( [ a , b ], n , 1 ) In Python: stats . randint . rvs ( a , b , size = n ) In Julia: rand ( a : b , n ) Binomial distribution with k k trials each having probability p p \u00b6 In R: rbinom ( n , k , p ) In MATLAB: random ( \"Binomial\" , k , p , n , 1 ) In Python: stats . binom . rvs ( k , p , size = n ) In Julia: rand ( Binomial ( k , p ), n ) Geometric distribution with success probability p p \u00b6 (specifically, the number of failures observed before a success was observed) In R: rgeom ( n , p ) In MATLAB: random ( \"Geometric\" , p , n , 1 ) In Python: stats . geom . rvs ( p , size = n ) In Julia: rand ( Geometric ( p ), n ) Poisson distribution with expected rate \\lambda \\lambda \u00b6 In R: rpois ( n , lambda ) In MATLAB: random ( \"Poisson\" , lambda , n , 1 ) In Python: stats . poisson ( lambda , size = n ) In Julia: rand ( Poisson ( lambda ), n ) Uniform distribution on the interval [a,b] [a,b] \u00b6 In R: runif ( n , a , b ) In MATLAB: random ( \"Uniform\" , a , b , n , 1 ) In Python: stats . uniform . rvs ( a , b , size = n ) In Julia: rand ( Uniform ( a , b ), n ) Triangular distribution on the interval [a,b] [a,b] with mode c c \u00b6 In R: (This assumes you have done install.packages('triangle') and library('triangle') .) rtriangle ( n , a , b , c ) In MATLAB this distribution is not built in. In Python: stats . triang . rvs ( ( c - a ) / ( b - a ), a , b - a , size = n ) Distributions.js does not contain this distribution. Normal distribution with mean \\mu \\mu and standard deviation \\sigma \\sigma \u00b6 In R: rnorm ( n , mu , sigma ) In MATLAB: random ( \"Normal\" , mu , sigma , n , 1 ) In Python: stats . norm . rvs ( mu , sigma , size = n ) In Julia: rand ( Normal ( mu , sigma ), n ) Exponential distribution with rate \\lambda \\lambda \u00b6 In R: rexp ( n , lambda ) In MATLAB: random ( \"Exponential\" , lambda , n , 1 ) In Python: stats . expon . rvs ( lambda , size = n ) In Julia: rand ( Exponential ( lambda ), n )","title":"Chapter 6, Operations Research"},{"location":"chapter-6-resources/#chapter-6-operations-research","text":"Authors: Alice Paul and Susan Martonosi Digital resources supplementing the chapter","title":"Chapter 6: Operations Research"},{"location":"chapter-6-resources/#sample-r-code","text":"R code for generating Figure 6.7 of the text Figure 6.7 shows two images: 1. a histogram of 1000 randomly generated U(0,1) U(0,1) values overlaid with the density of that distribution 2. a histogram of X=-\\frac{\\ln(1-U)}{\\lambda} X=-\\frac{\\ln(1-U)}{\\lambda} with \\lambda=5 \\lambda=5 created from the first distribution, with the exponential distribution (with \\lambda=5 \\lambda=5 ) overlaid R code for generating Table 6.1 of the text Table 6.1 illustrates several different probability distributions, including * discrete uniform, * binomial, * geometric, * Poisson, * uniform, * triangle, * normal, and * exponential. Each of those distributions can be accessed using code in a variety of languages, as shown below. The reader may also be interested in the individual documentation for probability distributions in R , MATLAB Python , or Julia .","title":"Sample R code"},{"location":"chapter-6-resources/#code-reference-for-random-number-generation","text":"In each example below, n n is the number of values to generate. All Python code assumes you have installed SciPy and have imported its statistics package with import scipy.stats as stats . All Julia code assumes you have installed Distributions.jl and have imported it with using Distributions .","title":"Code reference for random number generation"},{"location":"chapter-6-resources/#discrete-uniform-distribution-on-aa1ldotsbaa1ldotsb","text":"In R: sample ( a : b , n , replace = TRUE ) In MATLAB: randi ( [ a , b ], n , 1 ) In Python: stats . randint . rvs ( a , b , size = n ) In Julia: rand ( a : b , n )","title":"Discrete uniform distribution on \\{a,a+1,\\ldots,b\\}\\{a,a+1,\\ldots,b\\}"},{"location":"chapter-6-resources/#binomial-distribution-with-kk-trials-each-having-probability-pp","text":"In R: rbinom ( n , k , p ) In MATLAB: random ( \"Binomial\" , k , p , n , 1 ) In Python: stats . binom . rvs ( k , p , size = n ) In Julia: rand ( Binomial ( k , p ), n )","title":"Binomial distribution with kk trials each having probability pp"},{"location":"chapter-6-resources/#geometric-distribution-with-success-probability-pp","text":"(specifically, the number of failures observed before a success was observed) In R: rgeom ( n , p ) In MATLAB: random ( \"Geometric\" , p , n , 1 ) In Python: stats . geom . rvs ( p , size = n ) In Julia: rand ( Geometric ( p ), n )","title":"Geometric distribution with success probability pp"},{"location":"chapter-6-resources/#poisson-distribution-with-expected-rate-lambdalambda","text":"In R: rpois ( n , lambda ) In MATLAB: random ( \"Poisson\" , lambda , n , 1 ) In Python: stats . poisson ( lambda , size = n ) In Julia: rand ( Poisson ( lambda ), n )","title":"Poisson distribution with expected rate \\lambda\\lambda"},{"location":"chapter-6-resources/#uniform-distribution-on-the-interval-abab","text":"In R: runif ( n , a , b ) In MATLAB: random ( \"Uniform\" , a , b , n , 1 ) In Python: stats . uniform . rvs ( a , b , size = n ) In Julia: rand ( Uniform ( a , b ), n )","title":"Uniform distribution on the interval [a,b][a,b]"},{"location":"chapter-6-resources/#triangular-distribution-on-the-interval-abab-with-mode-cc","text":"In R: (This assumes you have done install.packages('triangle') and library('triangle') .) rtriangle ( n , a , b , c ) In MATLAB this distribution is not built in. In Python: stats . triang . rvs ( ( c - a ) / ( b - a ), a , b - a , size = n ) Distributions.js does not contain this distribution.","title":"Triangular distribution on the interval [a,b][a,b] with mode cc"},{"location":"chapter-6-resources/#normal-distribution-with-mean-mumu-and-standard-deviation-sigmasigma","text":"In R: rnorm ( n , mu , sigma ) In MATLAB: random ( \"Normal\" , mu , sigma , n , 1 ) In Python: stats . norm . rvs ( mu , sigma , size = n ) In Julia: rand ( Normal ( mu , sigma ), n )","title":"Normal distribution with mean \\mu\\mu and standard deviation \\sigma\\sigma"},{"location":"chapter-6-resources/#exponential-distribution-with-rate-lambdalambda","text":"In R: rexp ( n , lambda ) In MATLAB: random ( \"Exponential\" , lambda , n , 1 ) In Python: stats . expon . rvs ( lambda , size = n ) In Julia: rand ( Exponential ( lambda ), n )","title":"Exponential distribution with rate \\lambda\\lambda"},{"location":"chapter-7-resources/","text":"Chapter 7: Dimensionality Reduction \u00b6 Authors: Sofya Chepushtanova , Elin Farnell , Eric Kehoe , Michael Kirby , and Henry Kvinge Digital resources supplementing the chapter None here yet; this page is a placeholder. Check back later.","title":"Chapter 7, Dimensionality Reduction"},{"location":"chapter-7-resources/#chapter-7-dimensionality-reduction","text":"Authors: Sofya Chepushtanova , Elin Farnell , Eric Kehoe , Michael Kirby , and Henry Kvinge Digital resources supplementing the chapter None here yet; this page is a placeholder. Check back later.","title":"Chapter 7: Dimensionality Reduction"},{"location":"chapter-8-resources/","text":"Chapter 8: Machine Learning \u00b6 Authors: Mahesh Agarwal , Nathan Carter , and David Oury Digital resources supplementing the chapter Boston Housing Dataset Example \u00b6 The chapter includes code that loads a built-in scikit-learn dataset of Boston housing prices from 1978 and fits a linear model using appropriate train/test splitting and simple feature selection. (The dataset's origins are documented here .) Run Jupyter notebook online using Google Colab: View Jupyter notebook on GitHub View generated PDF","title":"Chapter 8, Machine Learning"},{"location":"chapter-8-resources/#chapter-8-machine-learning","text":"Authors: Mahesh Agarwal , Nathan Carter , and David Oury Digital resources supplementing the chapter","title":"Chapter 8: Machine Learning"},{"location":"chapter-8-resources/#boston-housing-dataset-example","text":"The chapter includes code that loads a built-in scikit-learn dataset of Boston housing prices from 1978 and fits a linear model using appropriate train/test splitting and simple feature selection. (The dataset's origins are documented here .) Run Jupyter notebook online using Google Colab: View Jupyter notebook on GitHub View generated PDF","title":"Boston Housing Dataset Example"},{"location":"chapter-9-resources/","text":"Chapter 9: Deep Learning \u00b6 Author: Samuel S. Watson Digital resources supplementing the chapter None here yet; this page is a placeholder. Check back later.","title":"Chapter 9, Deep Learning"},{"location":"chapter-9-resources/#chapter-9-deep-learning","text":"Author: Samuel S. Watson Digital resources supplementing the chapter None here yet; this page is a placeholder. Check back later.","title":"Chapter 9: Deep Learning"},{"location":"forward/","text":"Forward \u00b6 Special thanks to Michael Pearson , Executive Director of the MAA, for being willing to contribute the following Forward to the text, and letting us publish it online as well. He is uniquely positioned to make the case for the importance of this text at this time. As we often hear, we live in an era where data can be collected, stored, and processed at an unprecedented (and rapidly accelerating) scale. Whether or not that happens in a way that can properly be called science, however, is a critical issue for our society. The recently-concluded Roundtable on Data Science Postsecondary Education , held under the auspices of the Board on Mathematical Sciences and Analytics at the National Academies , brought together representatives from academia, professional societies, industry, and funding agencies to share perspectives on what tools our students need to participate in this space. Throughout these discussions, it was clear that, while there is no single model for what constitutes data science, there are a wide variety of tools from mathematics, statistics, and computer science which are essential ingredients for anyone interested in exploring this rapidly-evolving discipline. For those of us in mathematics, it is essential that we become better-informed about the role of our discipline in this emerging field. Doing so will help prepare our students for careers which will more and more depend on some level of competency in understanding how to use data to inform decisions, regardless of the specific discipline or industry in which they find themselves. It can also bring heightened awareness of the importance of rigorous mathematical perspectives to the enterprise. I believe it is an ethical, and in fact an existential, imperative for the mathematical sciences community to develop a deeper understanding of the role of our disciplines in data science, and to change our educational programs to enable our students to engage with data effectively, and with integrity. Nathan Carter and his colleagues have made an important contribution by providing an overview of many of the key tools from the mathematical, statistical, and computational sciences needed to succeed as a data scientist, written specifically for those of us in the mathematical sciences -- faculty interested in learning more about data science themselves, graduate students, and others with a reasonable level of mathematical maturity. From my perspective, for those of us concerned with undergraduate mathematics education, it cannot have come too soon.","title":"Forward"},{"location":"forward/#forward","text":"Special thanks to Michael Pearson , Executive Director of the MAA, for being willing to contribute the following Forward to the text, and letting us publish it online as well. He is uniquely positioned to make the case for the importance of this text at this time. As we often hear, we live in an era where data can be collected, stored, and processed at an unprecedented (and rapidly accelerating) scale. Whether or not that happens in a way that can properly be called science, however, is a critical issue for our society. The recently-concluded Roundtable on Data Science Postsecondary Education , held under the auspices of the Board on Mathematical Sciences and Analytics at the National Academies , brought together representatives from academia, professional societies, industry, and funding agencies to share perspectives on what tools our students need to participate in this space. Throughout these discussions, it was clear that, while there is no single model for what constitutes data science, there are a wide variety of tools from mathematics, statistics, and computer science which are essential ingredients for anyone interested in exploring this rapidly-evolving discipline. For those of us in mathematics, it is essential that we become better-informed about the role of our discipline in this emerging field. Doing so will help prepare our students for careers which will more and more depend on some level of competency in understanding how to use data to inform decisions, regardless of the specific discipline or industry in which they find themselves. It can also bring heightened awareness of the importance of rigorous mathematical perspectives to the enterprise. I believe it is an ethical, and in fact an existential, imperative for the mathematical sciences community to develop a deeper understanding of the role of our disciplines in data science, and to change our educational programs to enable our students to engage with data effectively, and with integrity. Nathan Carter and his colleagues have made an important contribution by providing an overview of many of the key tools from the mathematical, statistical, and computational sciences needed to succeed as a data scientist, written specifically for those of us in the mathematical sciences -- faculty interested in learning more about data science themselves, graduate students, and others with a reasonable level of mathematical maturity. From my perspective, for those of us concerned with undergraduate mathematics education, it cannot have come too soon.","title":"Forward"},{"location":"persistent-homology-tutorial-1/","text":"Persistent Homology Tutorial 1 of 4 \u00b6 (See also Part 2 , Part 3 , and Part 4 .) Introduction \u00b6 This tutorial accompanies the chapter on Toplogical Data Analysis in the book Data Science for Mathematicians . The tutorial uses code from Ripser by Ulrich Bauer (Bauer, 2015) and a variety of other places, referenced throughout the text. This first part of this tutorial (Persistent homology) requires no software download. However, you do need to download some datasets, which you can get from this folder in the book's GitHub repository . Persistent homology \u00b6 You can get the PDF slides for Henry's introduction to persistent homology here . We will be using Ripser in this tutorial. Some slides about how Ripser works are available here . Ripser in your browser - synthetic examples \u00b6 The easiest way to run Ripser is in a live demo in your browser, for which no installation (and in particular, no Python) is required. Try it out! . House example - distance matrix \u00b6 For example, we can use Ripser to compute the persistent homology of the Vietoris-Rips complex of the following 5 points in the plane. The distance matrix for these five points can be downloaded here . It contains the 5x5 distance matrix corresponding to the collection of 5 points in the plane shown above. The contents of the file are as follows. 0, 2.0000, 2.8284, 2.0000, 3.1623 2.0000, 0, 2.0000, 2.8284, 3.1623 2.8284, 2.0000, 0, 2.0000, 1.4142 2.0000, 2.8482, 2.0000, 0, 1.4142 3.1623, 3.1623, 1.4142, 1.4142, 0 At the Ripser live webpage, enter the following input (including choosing to upload that distance matrix file, as shown). Ripser should give the following output in your browser. Note that the 5 connected components merge into one, with merging events happening at scales \\sqrt 2 \\sqrt 2 and 2. There is a single 1-dimensional feature, beginning at scale parameter 2 and ending at scale parameter \\sqrt 8 \\sqrt 8 . House example - point cloud \u00b6 Instead of loading a distance matrix, it is also possible to to load in a set of points in Euclidean space, listed by their Euclidean coordinates. See, for example, this file . It is the 5\\times2 5\\times2 matrix corresponding to the same collection of 5 points in the plane. To input a point cloud in Ripser live, simply select the option \"point cloud\" instead of \"distance matrix.\" You will get the same output as above! Torus example \u00b6 The following example computes the persistent homology barcodes of a 20\\times20 20\\times20 grid of 400 points on the unit torus S^1\\times S^1 S^1\\times S^1 in \\mathbb{R}^4 \\mathbb{R}^4 , where a small amount of noise has been added to each point. You can download the point cloud data file here . Only a subset of the intervals are shown below. Note that the long barcodes (roughly between scale parameter 0.7 and 1.5) recover the homology of the torus, with a single connected component, with two 1-dimensional holes, and with a single 2-dimensional hole. Sphere example, with a Python script to create the points \u00b6 A set of points sampled uniformly at random from the unit sphere can be downloaded here . But you don't have to use that particular set of randomly generated points. You can generate your own, as follows. Download and run the Python script sphere_points.py . It will sample 500 points from the unit sphere S^2 S^2 in \\mathbb{R}^3 \\mathbb{R}^3 , and then save the point cloud to a text file named sphere_points.txt . (If you have downloaded the entire code repository, you may want to move that file into the point_clouds folder.) We can compute the persistent homology barcodes in Ripser. So that the computation finishes, we have asked Ripser to compute only up to scale parameter 1.2. (One can compute larger examples after downloading Ripser to one's machine, as in Part 3 of this tutorial.) Only a subset of the intervals are shown below. Note that the long barcodes (roughly between scale parameter 0.55 and 1.2) recover the homology of the 2-sphere, with a single connected component, no 1-dimensional holes, and a single 2-dimensional hole. Cyclooctane example \u00b6 This is an example with a real dataset of cyclooctane molecule conformations. The cyclooctane molecule consists of a ring of 8 carbon atoms (shown in black), each bonded to a pair of hydrogen atoms (shown in white). The cyclooctane molecule C 8 H 16 consists of a ring of 8 carbons atoms, each bonded to a pair of hydrogen atoms. A conformation of this molecule is a chemically and physically possible realization in 3D space, modulo translations and rotations. The locations of the carbon atoms in a conformation determine the locations of the hydrogen atoms via energy minimization, and hence each molecule conformation can be mapped to a point in \\mathbb{R}^{24}=\\mathbb{R}^{8\\times3} \\mathbb{R}^{24}=\\mathbb{R}^{8\\times3} , as there are eight carbon atoms in the molecule and each carbon location is represented by three coordinates x,y,z x,y,z . This map realizes the conformation space of cyclooctane as a subset of \\mathbb{R}^{24} \\mathbb{R}^{24} . It turns out that the conformation space is a two-dimensional stratefied space, i.e., a two-dimensional manifold with singularities. Furthermore, Brown et al. (2008), Martin et al. (2010), and Martin and Watson (2011) show that the conformation space of cyclooctane is the union of a sphere with a Klein bottle, glued together along two circles of singularities. (See Chapter 10 of Data Science for Mathematicians for more detail, or Figures 7 and 8 in Martin and Watson (2011).) Indeed, the algorithm they develop allows them to triangulate this conformation space from a finite sample. Zomorodian (2012) uses the cyclooctane dataset as an example to show that we can efficiently recover the homology groups of the conformation space using persistent homology. In this section we essentially follow Zomorodian's example. We begin with a sample of 1,000 points on the conformation space. (This data is publicly available at Shawn Martin's webpage .) We then compute the resulting persistent homology. We obtain the Betti numbers b_0=b_1=1 b_0=b_1=1 and b_2=2 b_2=2 , which match the homology groups of the union of a sphere with a Klein bottle, glued together along two circles of singularities. The following example computes the persistent homology barcodes of 1,000 points from the configuration space of cyclooctane molecules in \\mathbb{R}^{24} \\mathbb{R}^{24} . This dataset is available here . So that the computation finishes, we have asked Ripser to compute only up to scale parameter 1.3. (One can compute larger examples after downloading Ripser to one's machine, as in Part 3 of this tutorial.) Only a subset of the intervals are shown below. The long barcodes (roughly between scale parameter 0.65 and 1.25), with one connected component, one 1-dimensional hole, and two 2-dimensional holes, match the homology of the union of a sphere with a Klein bottle, glued together along two circles of singularities. Optical image patch example \u00b6 This is an example with a real dataset of optical image patch data. The optical image database collected by van Hateren and van der Schaaf (1998) contains black and white digital photographs from a variety of indoor and outdoor scenes. Lee et al. (2003) study 3\\times3 3\\times3 patches from these images, and Carlsson et al. (2008) continue the analysis of image patches using persistent homology. Carlsson et al. (2008) begin with a large collection of high-contrast, normalized 3\\times3 3\\times3 pixel patches, each thought of as a point in \\mathbb{R}^9 \\mathbb{R}^9 . They change to the Discrete Cosine Transform (DCT) basis, which maps the patches to the unit sphere S^7 S^7 in \\mathbb{R}^8 \\mathbb{R}^8 . They select from this space the 30% densest vectors, where density is based on the distance from a point to its 300 th nearest neighbor. In Carlsson et al. (2008), this dense core subset is denoted X(300,30) X(300,30) . In the next example we verify the result from Carlsson et al. (2008): X(300,30) X(300,30) has the topology of a circle. The following example computes the persistent homology barcodes of 1,000 points from X(300,30) X(300,30) . The dataset can be downloaded here . So that the computation finishes, we have asked Ripser to compute only up to scale parameter 1.3. (One can compute larger examples after downloading Ripser to one's machine, as in Part 3 of this tutorial.) Only a subset of the intervals are shown below. The long barcodes (roughly between scale parameter 0.5 and 1.25), with one connected component and one 1-dimensional hole, have the homology of a circle. This is good evidence that the core subset X(300,30) X(300,30) is well-approximated by a circle. We plot the projection of these points onto the first two linear gradient Discrete Cosine Transform basis vectors. The projection of X(300,30) X(300,30) above shows a circle. It is called the optical primary circle and is parameterized as shown below. We can also consider the 3-circle model, which arises from the dataset X(15,30) X(15,30) that corresponds to a more local estimate of density. The dataset can be downloaded here . So that the computation finishes, we select only 1,000 points from X(15,30) X(15,30) , and we have asked Ripser to compute only up to scale parameter 1.2. (One can compute larger examples after downloading Ripser to one's machine, as in Part 3 of this tutorial.) Only a subset of the intervals are shown below. The barcodes with one connected component and five 1-dimensional holes correspond to the 3-circle model for the core subset X(15,30) X(15,30) . Much more convincing (longer) intervals can be obtained by using more points and computing with Ripser downloaded to your local machine. We plot the projection of these points onto the first two linear gradient Discrete Cosine Transform basis vectors. The corresponding three-circle model for optical images is shown below. (See also Part 2 , Part 3 , and Part 4 .)","title":"Persistent Homology Tutorial 1 of 4"},{"location":"persistent-homology-tutorial-1/#persistent-homology-tutorial-1-of-4","text":"(See also Part 2 , Part 3 , and Part 4 .)","title":"Persistent Homology Tutorial 1 of 4"},{"location":"persistent-homology-tutorial-1/#introduction","text":"This tutorial accompanies the chapter on Toplogical Data Analysis in the book Data Science for Mathematicians . The tutorial uses code from Ripser by Ulrich Bauer (Bauer, 2015) and a variety of other places, referenced throughout the text. This first part of this tutorial (Persistent homology) requires no software download. However, you do need to download some datasets, which you can get from this folder in the book's GitHub repository .","title":"Introduction"},{"location":"persistent-homology-tutorial-1/#persistent-homology","text":"You can get the PDF slides for Henry's introduction to persistent homology here . We will be using Ripser in this tutorial. Some slides about how Ripser works are available here .","title":"Persistent homology"},{"location":"persistent-homology-tutorial-1/#ripser-in-your-browser-synthetic-examples","text":"The easiest way to run Ripser is in a live demo in your browser, for which no installation (and in particular, no Python) is required. Try it out! .","title":"Ripser in your browser - synthetic examples"},{"location":"persistent-homology-tutorial-1/#house-example-distance-matrix","text":"For example, we can use Ripser to compute the persistent homology of the Vietoris-Rips complex of the following 5 points in the plane. The distance matrix for these five points can be downloaded here . It contains the 5x5 distance matrix corresponding to the collection of 5 points in the plane shown above. The contents of the file are as follows. 0, 2.0000, 2.8284, 2.0000, 3.1623 2.0000, 0, 2.0000, 2.8284, 3.1623 2.8284, 2.0000, 0, 2.0000, 1.4142 2.0000, 2.8482, 2.0000, 0, 1.4142 3.1623, 3.1623, 1.4142, 1.4142, 0 At the Ripser live webpage, enter the following input (including choosing to upload that distance matrix file, as shown). Ripser should give the following output in your browser. Note that the 5 connected components merge into one, with merging events happening at scales \\sqrt 2 \\sqrt 2 and 2. There is a single 1-dimensional feature, beginning at scale parameter 2 and ending at scale parameter \\sqrt 8 \\sqrt 8 .","title":"House example - distance matrix"},{"location":"persistent-homology-tutorial-1/#house-example-point-cloud","text":"Instead of loading a distance matrix, it is also possible to to load in a set of points in Euclidean space, listed by their Euclidean coordinates. See, for example, this file . It is the 5\\times2 5\\times2 matrix corresponding to the same collection of 5 points in the plane. To input a point cloud in Ripser live, simply select the option \"point cloud\" instead of \"distance matrix.\" You will get the same output as above!","title":"House example - point cloud"},{"location":"persistent-homology-tutorial-1/#torus-example","text":"The following example computes the persistent homology barcodes of a 20\\times20 20\\times20 grid of 400 points on the unit torus S^1\\times S^1 S^1\\times S^1 in \\mathbb{R}^4 \\mathbb{R}^4 , where a small amount of noise has been added to each point. You can download the point cloud data file here . Only a subset of the intervals are shown below. Note that the long barcodes (roughly between scale parameter 0.7 and 1.5) recover the homology of the torus, with a single connected component, with two 1-dimensional holes, and with a single 2-dimensional hole.","title":"Torus example"},{"location":"persistent-homology-tutorial-1/#sphere-example-with-a-python-script-to-create-the-points","text":"A set of points sampled uniformly at random from the unit sphere can be downloaded here . But you don't have to use that particular set of randomly generated points. You can generate your own, as follows. Download and run the Python script sphere_points.py . It will sample 500 points from the unit sphere S^2 S^2 in \\mathbb{R}^3 \\mathbb{R}^3 , and then save the point cloud to a text file named sphere_points.txt . (If you have downloaded the entire code repository, you may want to move that file into the point_clouds folder.) We can compute the persistent homology barcodes in Ripser. So that the computation finishes, we have asked Ripser to compute only up to scale parameter 1.2. (One can compute larger examples after downloading Ripser to one's machine, as in Part 3 of this tutorial.) Only a subset of the intervals are shown below. Note that the long barcodes (roughly between scale parameter 0.55 and 1.2) recover the homology of the 2-sphere, with a single connected component, no 1-dimensional holes, and a single 2-dimensional hole.","title":"Sphere example, with a Python script to create the points"},{"location":"persistent-homology-tutorial-1/#cyclooctane-example","text":"This is an example with a real dataset of cyclooctane molecule conformations. The cyclooctane molecule consists of a ring of 8 carbon atoms (shown in black), each bonded to a pair of hydrogen atoms (shown in white). The cyclooctane molecule C 8 H 16 consists of a ring of 8 carbons atoms, each bonded to a pair of hydrogen atoms. A conformation of this molecule is a chemically and physically possible realization in 3D space, modulo translations and rotations. The locations of the carbon atoms in a conformation determine the locations of the hydrogen atoms via energy minimization, and hence each molecule conformation can be mapped to a point in \\mathbb{R}^{24}=\\mathbb{R}^{8\\times3} \\mathbb{R}^{24}=\\mathbb{R}^{8\\times3} , as there are eight carbon atoms in the molecule and each carbon location is represented by three coordinates x,y,z x,y,z . This map realizes the conformation space of cyclooctane as a subset of \\mathbb{R}^{24} \\mathbb{R}^{24} . It turns out that the conformation space is a two-dimensional stratefied space, i.e., a two-dimensional manifold with singularities. Furthermore, Brown et al. (2008), Martin et al. (2010), and Martin and Watson (2011) show that the conformation space of cyclooctane is the union of a sphere with a Klein bottle, glued together along two circles of singularities. (See Chapter 10 of Data Science for Mathematicians for more detail, or Figures 7 and 8 in Martin and Watson (2011).) Indeed, the algorithm they develop allows them to triangulate this conformation space from a finite sample. Zomorodian (2012) uses the cyclooctane dataset as an example to show that we can efficiently recover the homology groups of the conformation space using persistent homology. In this section we essentially follow Zomorodian's example. We begin with a sample of 1,000 points on the conformation space. (This data is publicly available at Shawn Martin's webpage .) We then compute the resulting persistent homology. We obtain the Betti numbers b_0=b_1=1 b_0=b_1=1 and b_2=2 b_2=2 , which match the homology groups of the union of a sphere with a Klein bottle, glued together along two circles of singularities. The following example computes the persistent homology barcodes of 1,000 points from the configuration space of cyclooctane molecules in \\mathbb{R}^{24} \\mathbb{R}^{24} . This dataset is available here . So that the computation finishes, we have asked Ripser to compute only up to scale parameter 1.3. (One can compute larger examples after downloading Ripser to one's machine, as in Part 3 of this tutorial.) Only a subset of the intervals are shown below. The long barcodes (roughly between scale parameter 0.65 and 1.25), with one connected component, one 1-dimensional hole, and two 2-dimensional holes, match the homology of the union of a sphere with a Klein bottle, glued together along two circles of singularities.","title":"Cyclooctane example"},{"location":"persistent-homology-tutorial-1/#optical-image-patch-example","text":"This is an example with a real dataset of optical image patch data. The optical image database collected by van Hateren and van der Schaaf (1998) contains black and white digital photographs from a variety of indoor and outdoor scenes. Lee et al. (2003) study 3\\times3 3\\times3 patches from these images, and Carlsson et al. (2008) continue the analysis of image patches using persistent homology. Carlsson et al. (2008) begin with a large collection of high-contrast, normalized 3\\times3 3\\times3 pixel patches, each thought of as a point in \\mathbb{R}^9 \\mathbb{R}^9 . They change to the Discrete Cosine Transform (DCT) basis, which maps the patches to the unit sphere S^7 S^7 in \\mathbb{R}^8 \\mathbb{R}^8 . They select from this space the 30% densest vectors, where density is based on the distance from a point to its 300 th nearest neighbor. In Carlsson et al. (2008), this dense core subset is denoted X(300,30) X(300,30) . In the next example we verify the result from Carlsson et al. (2008): X(300,30) X(300,30) has the topology of a circle. The following example computes the persistent homology barcodes of 1,000 points from X(300,30) X(300,30) . The dataset can be downloaded here . So that the computation finishes, we have asked Ripser to compute only up to scale parameter 1.3. (One can compute larger examples after downloading Ripser to one's machine, as in Part 3 of this tutorial.) Only a subset of the intervals are shown below. The long barcodes (roughly between scale parameter 0.5 and 1.25), with one connected component and one 1-dimensional hole, have the homology of a circle. This is good evidence that the core subset X(300,30) X(300,30) is well-approximated by a circle. We plot the projection of these points onto the first two linear gradient Discrete Cosine Transform basis vectors. The projection of X(300,30) X(300,30) above shows a circle. It is called the optical primary circle and is parameterized as shown below. We can also consider the 3-circle model, which arises from the dataset X(15,30) X(15,30) that corresponds to a more local estimate of density. The dataset can be downloaded here . So that the computation finishes, we select only 1,000 points from X(15,30) X(15,30) , and we have asked Ripser to compute only up to scale parameter 1.2. (One can compute larger examples after downloading Ripser to one's machine, as in Part 3 of this tutorial.) Only a subset of the intervals are shown below. The barcodes with one connected component and five 1-dimensional holes correspond to the 3-circle model for the core subset X(15,30) X(15,30) . Much more convincing (longer) intervals can be obtained by using more points and computing with Ripser downloaded to your local machine. We plot the projection of these points onto the first two linear gradient Discrete Cosine Transform basis vectors. The corresponding three-circle model for optical images is shown below. (See also Part 2 , Part 3 , and Part 4 .)","title":"Optical image patch example"},{"location":"persistent-homology-tutorial-2/","text":"Persistent Homology Tutorial 2 of 4 \u00b6 (See also Part 1 , Part 3 , and Part 4 .) Exercises on persistent homology \u00b6 Exercise : Write a script (say in Python, or some other language) that selects 400 points uniformly at random (or approximately uniformly at random) from the annulus $$ \\left\\{ (x,y)\\mid 0.95^2\\leq x^2+y^2\\leq1.05^2 \\right\\} $$ in \\mathbb{R}^2 \\mathbb{R}^2 . Compute its persistent homology barcodes using Ripser. Exercise : Write a script (say in Python, or some other language) that selects 400 points uniformly at random (or approximately uniformly at random) from the \"coconut shell\" $$ \\left\\{ (x,y,z) \\mid 0.95^2\\leq x^2+y^2+z^2\\leq 1.05^2 \\right\\} $$ in \\mathbb{R}^3 \\mathbb{R}^3 . Compute its persistent homology barcodes using Ripser. Exercise : Write a Python script (or code in some other language) that will select n n evenly-spaced points from the unit circle in the plane. Compute the persistent homology of the Vietoris-Rips complex of 4, 6, 9, 12, 15, and 20 equally spaced points on the circle. Do you get ever homology above dimension 1? Exercise : Find a planar dataset Z Z in \\mathbb{R}^2 \\mathbb{R}^2 and a filtration value t t such that VR(Z,t) VR(Z,t) has nonzero Betti number b_2 b_2 . Do a computation in Ripser to confirm your answer. Exercise : Find a planar dataset Z Z in \\mathbb{R}^2 \\mathbb{R}^2 and a filtration value t t such that VR(Z,t) VR(Z,t) has nonzero Betti number b_6 b_6 . Do a computation in Ripser to confirm your answer. Exercise : Let X X be the 8 vertices of the cube in \\mathbb{R}^3 \\mathbb{R}^3 , X=\\{(\\pm 1,\\pm 1,\\pm 1)\\} X=\\{(\\pm 1,\\pm 1,\\pm 1)\\} . Equip X X with the Euclidean metric. Compute the persistent homology of the Vietoris-Rips complex of X X . Do you get ever homology above dimension 2? Exercise : One way to produce a torus is to take a square [0, 1] \\times [0, 1] [0, 1] \\times [0, 1] and then identify opposite sides. This is called a flat torus. More explicitly, the flat torus is the quotient space ([0, 1] \\times [0, 1]) / \\sim ([0, 1] \\times [0, 1]) / \\sim , where (0, y) \\sim (1, y) (0, y) \\sim (1, y) for all y y in [0, 1] [0, 1] and where (x, 0) \\sim (x, 1) (x, 0) \\sim (x, 1) for all x x in [0, 1] [0, 1] . The Euclidean metric on [0, 1] \\times [0, 1] [0, 1] \\times [0, 1] induces a metric on the flat torus. For example, in the induced metric on the flat torus, the distance between (0, 1/2) (0, 1/2) and (1, 1/2) (1, 1/2) is zero, since these two points are identified. The distance between (1/10, 1/2) (1/10, 1/2) and (9/10, 1/2) (9/10, 1/2) is 2/10 2/10 , by passing through the point (0, 1/2) \\sim (1, 1/2) (0, 1/2) \\sim (1, 1/2) . Write a Python script (or code in another language) that selects 400 random points from the square [0, 1] \\times [0, 1] [0, 1] \\times [0, 1] and then computes the 400\\times400 400\\times400 distance matrix for these points under the induced metric on the flat torus. Use Ripser to compute the persistent homology of this metric space. Exercise : One way to produce a Klein bottle is to take a square [0, 1] \\times [0, 1] [0, 1] \\times [0, 1] and then identify opposite edges, with the left and right sides identified with a twist. This is called a flat Klein bottle. More explicitly, the flat Klein bottle is the quotient space ([0, 1] \\times [0, 1]) / \\sim ([0, 1] \\times [0, 1]) / \\sim , where (0, y) \\sim (1, 1 - y) (0, y) \\sim (1, 1 - y) for all y y in [0, 1] [0, 1] and where (x, 0) \\sim (x, 1) (x, 0) \\sim (x, 1) for all x x in [0, 1] [0, 1] . The Euclidean metric on [0, 1] \\times [0, 1] [0, 1] \\times [0, 1] induces a metric on the flat Klein bottle. For example, in the induced metric on the flat Klein bottle, the distance between (0, 4/10) (0, 4/10) and (1, 6/10) (1, 6/10) is zero, since these two points are identified. The distance between (1/10, 4/10) (1/10, 4/10) and (9/10, 6/10) (9/10, 6/10) is 2/10 2/10 , by passing through the point (0, 4/10) \\sim (1, 6/10) (0, 4/10) \\sim (1, 6/10) . Write a Python script (or code in another language) that selects 400 random points from the square [0, 1] \\times [0, 1] [0, 1] \\times [0, 1] and then computes the 400\\times400 400\\times400 distance matrix for these points under the induced metric on the flat Klein bottle. Use Ripser to compute the persistent homology of this metric space. If you have Ripser installed on your local machine, change to \\mathbb{Z}/3\\mathbb{Z} \\mathbb{Z}/3\\mathbb{Z} coefficients and see how the persistent homology changes. Exercise : One way to produce a projective plane is to take the unit sphere S^2 S^2 in \\mathbb{R}^3 \\mathbb{R}^3 and then identify antipodal points. More explicitly, the projective plane is the quotient space S^2 / (x \\sim -x) S^2 / (x \\sim -x) . The Euclidean metric on S^2 S^2 induces a metric on the projective plane. Write a Python script (or code in another language) that selects 400 random points from the unit sphere S^2 S^2 in \\mathbb{R}^3 \\mathbb{R}^3 and then computes the 400\\times400 400\\times400 distance matrix for these points under the induced metric on the projective plane. Use Ripser to compute the persistent homology of this metric space. If you have Ripser installed on your local machine, change to \\mathbb{Z}/3\\mathbb{Z} \\mathbb{Z}/3\\mathbb{Z} coefficients and see how the persistent homology changes. (See also Part 1 , Part 3 , and Part 4 .)","title":"Persistent Homology Tutorial 2 of 4"},{"location":"persistent-homology-tutorial-2/#persistent-homology-tutorial-2-of-4","text":"(See also Part 1 , Part 3 , and Part 4 .)","title":"Persistent Homology Tutorial 2 of 4"},{"location":"persistent-homology-tutorial-2/#exercises-on-persistent-homology","text":"Exercise : Write a script (say in Python, or some other language) that selects 400 points uniformly at random (or approximately uniformly at random) from the annulus $$ \\left\\{ (x,y)\\mid 0.95^2\\leq x^2+y^2\\leq1.05^2 \\right\\} $$ in \\mathbb{R}^2 \\mathbb{R}^2 . Compute its persistent homology barcodes using Ripser. Exercise : Write a script (say in Python, or some other language) that selects 400 points uniformly at random (or approximately uniformly at random) from the \"coconut shell\" $$ \\left\\{ (x,y,z) \\mid 0.95^2\\leq x^2+y^2+z^2\\leq 1.05^2 \\right\\} $$ in \\mathbb{R}^3 \\mathbb{R}^3 . Compute its persistent homology barcodes using Ripser. Exercise : Write a Python script (or code in some other language) that will select n n evenly-spaced points from the unit circle in the plane. Compute the persistent homology of the Vietoris-Rips complex of 4, 6, 9, 12, 15, and 20 equally spaced points on the circle. Do you get ever homology above dimension 1? Exercise : Find a planar dataset Z Z in \\mathbb{R}^2 \\mathbb{R}^2 and a filtration value t t such that VR(Z,t) VR(Z,t) has nonzero Betti number b_2 b_2 . Do a computation in Ripser to confirm your answer. Exercise : Find a planar dataset Z Z in \\mathbb{R}^2 \\mathbb{R}^2 and a filtration value t t such that VR(Z,t) VR(Z,t) has nonzero Betti number b_6 b_6 . Do a computation in Ripser to confirm your answer. Exercise : Let X X be the 8 vertices of the cube in \\mathbb{R}^3 \\mathbb{R}^3 , X=\\{(\\pm 1,\\pm 1,\\pm 1)\\} X=\\{(\\pm 1,\\pm 1,\\pm 1)\\} . Equip X X with the Euclidean metric. Compute the persistent homology of the Vietoris-Rips complex of X X . Do you get ever homology above dimension 2? Exercise : One way to produce a torus is to take a square [0, 1] \\times [0, 1] [0, 1] \\times [0, 1] and then identify opposite sides. This is called a flat torus. More explicitly, the flat torus is the quotient space ([0, 1] \\times [0, 1]) / \\sim ([0, 1] \\times [0, 1]) / \\sim , where (0, y) \\sim (1, y) (0, y) \\sim (1, y) for all y y in [0, 1] [0, 1] and where (x, 0) \\sim (x, 1) (x, 0) \\sim (x, 1) for all x x in [0, 1] [0, 1] . The Euclidean metric on [0, 1] \\times [0, 1] [0, 1] \\times [0, 1] induces a metric on the flat torus. For example, in the induced metric on the flat torus, the distance between (0, 1/2) (0, 1/2) and (1, 1/2) (1, 1/2) is zero, since these two points are identified. The distance between (1/10, 1/2) (1/10, 1/2) and (9/10, 1/2) (9/10, 1/2) is 2/10 2/10 , by passing through the point (0, 1/2) \\sim (1, 1/2) (0, 1/2) \\sim (1, 1/2) . Write a Python script (or code in another language) that selects 400 random points from the square [0, 1] \\times [0, 1] [0, 1] \\times [0, 1] and then computes the 400\\times400 400\\times400 distance matrix for these points under the induced metric on the flat torus. Use Ripser to compute the persistent homology of this metric space. Exercise : One way to produce a Klein bottle is to take a square [0, 1] \\times [0, 1] [0, 1] \\times [0, 1] and then identify opposite edges, with the left and right sides identified with a twist. This is called a flat Klein bottle. More explicitly, the flat Klein bottle is the quotient space ([0, 1] \\times [0, 1]) / \\sim ([0, 1] \\times [0, 1]) / \\sim , where (0, y) \\sim (1, 1 - y) (0, y) \\sim (1, 1 - y) for all y y in [0, 1] [0, 1] and where (x, 0) \\sim (x, 1) (x, 0) \\sim (x, 1) for all x x in [0, 1] [0, 1] . The Euclidean metric on [0, 1] \\times [0, 1] [0, 1] \\times [0, 1] induces a metric on the flat Klein bottle. For example, in the induced metric on the flat Klein bottle, the distance between (0, 4/10) (0, 4/10) and (1, 6/10) (1, 6/10) is zero, since these two points are identified. The distance between (1/10, 4/10) (1/10, 4/10) and (9/10, 6/10) (9/10, 6/10) is 2/10 2/10 , by passing through the point (0, 4/10) \\sim (1, 6/10) (0, 4/10) \\sim (1, 6/10) . Write a Python script (or code in another language) that selects 400 random points from the square [0, 1] \\times [0, 1] [0, 1] \\times [0, 1] and then computes the 400\\times400 400\\times400 distance matrix for these points under the induced metric on the flat Klein bottle. Use Ripser to compute the persistent homology of this metric space. If you have Ripser installed on your local machine, change to \\mathbb{Z}/3\\mathbb{Z} \\mathbb{Z}/3\\mathbb{Z} coefficients and see how the persistent homology changes. Exercise : One way to produce a projective plane is to take the unit sphere S^2 S^2 in \\mathbb{R}^3 \\mathbb{R}^3 and then identify antipodal points. More explicitly, the projective plane is the quotient space S^2 / (x \\sim -x) S^2 / (x \\sim -x) . The Euclidean metric on S^2 S^2 induces a metric on the projective plane. Write a Python script (or code in another language) that selects 400 random points from the unit sphere S^2 S^2 in \\mathbb{R}^3 \\mathbb{R}^3 and then computes the 400\\times400 400\\times400 distance matrix for these points under the induced metric on the projective plane. Use Ripser to compute the persistent homology of this metric space. If you have Ripser installed on your local machine, change to \\mathbb{Z}/3\\mathbb{Z} \\mathbb{Z}/3\\mathbb{Z} coefficients and see how the persistent homology changes. (See also Part 1 , Part 3 , and Part 4 .)","title":"Exercises on persistent homology"},{"location":"persistent-homology-tutorial-3/","text":"Persistent Homology Tutorial 3 of 4 \u00b6 (See also Part 1 , Part 2 , and Part 4 .) Ripser on your machine \u00b6 A more advanced (but very useful) step is to now download Ripser to your machine and to run it locally. This allows you to perform larger computations. Ripser is written in C++. You may download the code for Ripser here , which also contains installation instructions. Minimal installation instructions are listed below. git clone https://github.com/Ripser/ripser.git cd ripser make all ./ripser examples/sphere_3_192.lower_distance_matrix For convenience, you may want to download all the content of this folder and copy the executable file ripser into that folder, then cd into the same folder. You can use the flag --format distances to specify you are computing on a distance matrix, or --format point-cloud to specify you are computing on a point cloud. The flag --dim k specifies that homology is computed only up to dimension k k , and the flag --threshold t specifies that persistent homology is computed only up to scale parameter t t . For example, we can recreate all of the examples from Part 1 with the following commands. House example on the distance matrix: ./ripser --format distance distance_matrices/house_distances.txt House example on the point cloud: ./ripser --format point-cloud point_clouds/house_points.txt Torus example, up to 2-dimensional homology: ./ripser --format point-cloud --dim 2 point_clouds/torus_points.txt Sphere example, up to scale parameter 1.2: ./ripser --format point-cloud --dim 2 --threshold 1.2 point_clouds/sphere_points.txt Cyclooctane example. Try increasing the distance threshold gradually and see if your computer can do better than Ripser in your browser: ./ripser --format point-cloud --dim 2 --threshold 1.3 point_clouds/cyclooctane_points.txt Optical image patch example: ./ripser --format point-cloud --dim 1 --threshold 1.3 point_clouds/optical_k300_points.txt Instead of just printing Ripser's output to the terminal, you can also save it to a text file. The below example also saves the output to the text file house_points_ripser_printed.txt . ./ripser --format point-cloud point_clouds/house_points.txt | tee -a house_points_ripser_printed.txt Python \u00b6 The remainder of this tutorial requires one to have Python and/or the Ripser software package installed. Installing and running new code can be frustrating, especially if it is in a language (perhaps Python) that is unfamiliar to you. Nevertheless, we believe that it is extremely important for all practitioners of machine learning to have some exposure to Python. For this reason, the time you spend getting Python running on your machine is time well spent, even though this can feel like a frustrating investment of time. If you don't yet have Python, and if you are a PC user, then we recommend installing Anaconda . If you don't yet have Python, and if you are a Mac user, then we recommend installing Python 2.7.15 from here . We expect the code to work with an existing version of Python that you may happen to have already. Ripser with Python \u00b6 Melissa McGuirl has written very nice code for using Ripser with Python, which is what we will use in this section. In particular, the file house_points_ripser_printed.txt that we saved in Part 1 is not in a format that is terribly easy to work with. Melissa's code reformats Ripser output in a convenient manner. Alternatively, there is a Cython wrapper for Ripser available which might be more efficient and better for non-Linux machines. The wrapper is available here or here . Ensure you are in the folder of data files mentioned above and copy the Ripser executable into that folder as suggested earlier. (Or, alternatively, make sure that Ripser is in your Python path, and then in line 45 of getBarCodes.py , change ./ripser to ripser ). In your terminal, try running the following command. python getBarCodes.py -i distance_matrices/ -o ripser_outputs/ This will take every distance matrix in the folder distance_matrices , compute the persistent homology barcodes for the Vietoris-Rips complex built on top of this metric space, and print the output barcodes to the folder ripser_outputs . The following command then separates the Ripser output into barcode intervals separated by dimension. python separateRipser.py -i ripser_outputs/ -o barcodes/ And the following command then plots the corresponding persistence diagrams in your current directory. python plotpd.py -i barcodes/ -o ./ Melissa's code is written to work only with input metric space data in the form of a distance matrix, but one could edit it to also work with input metric space data in the form of a point cloud, for example. (See also Part 1 , Part 2 , and Part 4 .)","title":"Persistent Homology Tutorial 3 of 4"},{"location":"persistent-homology-tutorial-3/#persistent-homology-tutorial-3-of-4","text":"(See also Part 1 , Part 2 , and Part 4 .)","title":"Persistent Homology Tutorial 3 of 4"},{"location":"persistent-homology-tutorial-3/#ripser-on-your-machine","text":"A more advanced (but very useful) step is to now download Ripser to your machine and to run it locally. This allows you to perform larger computations. Ripser is written in C++. You may download the code for Ripser here , which also contains installation instructions. Minimal installation instructions are listed below. git clone https://github.com/Ripser/ripser.git cd ripser make all ./ripser examples/sphere_3_192.lower_distance_matrix For convenience, you may want to download all the content of this folder and copy the executable file ripser into that folder, then cd into the same folder. You can use the flag --format distances to specify you are computing on a distance matrix, or --format point-cloud to specify you are computing on a point cloud. The flag --dim k specifies that homology is computed only up to dimension k k , and the flag --threshold t specifies that persistent homology is computed only up to scale parameter t t . For example, we can recreate all of the examples from Part 1 with the following commands. House example on the distance matrix: ./ripser --format distance distance_matrices/house_distances.txt House example on the point cloud: ./ripser --format point-cloud point_clouds/house_points.txt Torus example, up to 2-dimensional homology: ./ripser --format point-cloud --dim 2 point_clouds/torus_points.txt Sphere example, up to scale parameter 1.2: ./ripser --format point-cloud --dim 2 --threshold 1.2 point_clouds/sphere_points.txt Cyclooctane example. Try increasing the distance threshold gradually and see if your computer can do better than Ripser in your browser: ./ripser --format point-cloud --dim 2 --threshold 1.3 point_clouds/cyclooctane_points.txt Optical image patch example: ./ripser --format point-cloud --dim 1 --threshold 1.3 point_clouds/optical_k300_points.txt Instead of just printing Ripser's output to the terminal, you can also save it to a text file. The below example also saves the output to the text file house_points_ripser_printed.txt . ./ripser --format point-cloud point_clouds/house_points.txt | tee -a house_points_ripser_printed.txt","title":"Ripser on your machine"},{"location":"persistent-homology-tutorial-3/#python","text":"The remainder of this tutorial requires one to have Python and/or the Ripser software package installed. Installing and running new code can be frustrating, especially if it is in a language (perhaps Python) that is unfamiliar to you. Nevertheless, we believe that it is extremely important for all practitioners of machine learning to have some exposure to Python. For this reason, the time you spend getting Python running on your machine is time well spent, even though this can feel like a frustrating investment of time. If you don't yet have Python, and if you are a PC user, then we recommend installing Anaconda . If you don't yet have Python, and if you are a Mac user, then we recommend installing Python 2.7.15 from here . We expect the code to work with an existing version of Python that you may happen to have already.","title":"Python"},{"location":"persistent-homology-tutorial-3/#ripser-with-python","text":"Melissa McGuirl has written very nice code for using Ripser with Python, which is what we will use in this section. In particular, the file house_points_ripser_printed.txt that we saved in Part 1 is not in a format that is terribly easy to work with. Melissa's code reformats Ripser output in a convenient manner. Alternatively, there is a Cython wrapper for Ripser available which might be more efficient and better for non-Linux machines. The wrapper is available here or here . Ensure you are in the folder of data files mentioned above and copy the Ripser executable into that folder as suggested earlier. (Or, alternatively, make sure that Ripser is in your Python path, and then in line 45 of getBarCodes.py , change ./ripser to ripser ). In your terminal, try running the following command. python getBarCodes.py -i distance_matrices/ -o ripser_outputs/ This will take every distance matrix in the folder distance_matrices , compute the persistent homology barcodes for the Vietoris-Rips complex built on top of this metric space, and print the output barcodes to the folder ripser_outputs . The following command then separates the Ripser output into barcode intervals separated by dimension. python separateRipser.py -i ripser_outputs/ -o barcodes/ And the following command then plots the corresponding persistence diagrams in your current directory. python plotpd.py -i barcodes/ -o ./ Melissa's code is written to work only with input metric space data in the form of a distance matrix, but one could edit it to also work with input metric space data in the form of a point cloud, for example. (See also Part 1 , Part 2 , and Part 4 .)","title":"Ripser with Python"},{"location":"persistent-homology-tutorial-4/","text":"Persistent Homology Tutorial 4 of 4 \u00b6 (See also Part 1 , Part 2 , and Part 3 .) Topological feature vectors - persistent homology and machine learning \u00b6 There are by now a wide variety of ways to incorporate persistent homology information into feature vectors for a machine learning task (Adcock et al 2016, Adams et al 2017, Atienza 2018, Bendich et al 2016, Bubenik 2015, Bubenik & Dlotko 2016, Carriere et al 2015, Carriere & Bauer 2018, Carriere et al 2018, Chazal & Divol 2018, Chen et al 2015, Chevyrev et al 2018, Chung et al 2009, Di Fabio & Ferri 2015, Hofer et al 2017, Kalisnik 2018, Reininghaus et al 2015, Skraba 2018, Topaz et al 2015, Zeppelzauer et al 2016). We tersely describe a few of these approaches. For Henry's \"from persistent homology to machine learning\" survey slides from the ICERM TRIPODS bootcamp, please see here . Persistence landscpaes \u00b6 One example is persistence landscapes (Bubenik 2015). Bubenik & Dlotko (2017) have made their code available , and persistence landscapes are also implementable in R-TDA . Persistence images \u00b6 A second example is persistence images (Adams et al 2017), with Python code available from Nathaniel Saul (Persims) or Francis Motta , and with Matlab code available here . We'll attempt to describe how to compute persistence images using the Jupyter notebooks accompanying Nathaniel Saul's Persim code . First, download the Persim code. Second, install Jupyter , perhaps using Anaconda as described in that link. Open a new terminal (which is often required after new installations) and in that terminal, try the following command. jupyter notebook This should open a window in an internet browser. In this browser window, change directories to persim/notebooks , and then open the Jupyter notebook Persistence Images.ipynb . You should then be able to run the code in the browser! Topological feature vectors - coding challenges \u00b6 6 shape classes \u00b6 Change directory to the folder topological-data-analysis/topological-feature-vectors/data-6-shape-classes . NOTE: Folder missing--must fix! This subfolder contains persistence diagram data for 6 shape classes: A unit cube A circle of diameter one A sphere of diameter one Three clusters with centers randomly chosen in the unit cube Three clusters within three clusters A torus with a major diameter of one and a minor diameter of one half. These shape classes are described in Section 6.1 of the paper Persistence Images: A Stable Vector Representation of Persistent Homology , and are shown below. We produced 25 point clouds of 500 randomly sampled points from each shape class. We then added a level of Gaussian noise to each point, at a noise level neta=0.1 or neta=0.05. We then have already computed the persistent homology intervals in homological dimension i=0 and i=1. Should \"neta\" here be \"eta\" (Greek letter)? If so, we can then use MathJax on these equations. For example, the file ToyData_PD_n05_23_6_0.txt corresponds to noise level neta=0.05, the 23 rd point cloud randomly sampled from shape class 6., with persistent homology computed in dimension 0. Each row of this file has two entries: the birth and death time of a 0-dimensional persistent homology interval. By contrast, the file ToyData_PD_n1_21_3_1.txt corresponds to noise level neta=1, the 21 st point cloud randomly sampled from shape class 3., with persistent homology computed in dimension 1. Each row of this file has two entries: the birth and death time of a 1-dimensional persistent homology interval. Your task is to use machine learning to distinguish these six classes from each other. In a K-medoids clustering test, some accuracies and computation times are displayed for bottleneck distances, Wasserstein distances, persistence landscapes, and persistence images in Table 1 of Persistence Images: A Stable Vector Representation of Persistent Homology . Do you have ideas for beating these accuracies or computation times? In the above table, PD means the persistence diagram equipped with either the L^1 L^1 (1-Wasserstein), L^2 L^2 (2-Wasserstein), or sup (bottleneck) metrics. PL means the persistence landscape, equipped with either the L^1 L^1 , L^2 L^2 , or sup metric. PI means the persistence image, equipped with either the L^1 L^1 , L^2 L^2 , or sup metric. But you should develop your own techniques for turning barcodes into feature vectors, and see how they compare! Some Matlab code for creating points from these shape classes is available here . Bibliography \u00b6 A. Adcock, E. Carlsson, and G. Carlsson. The ring of algebraic functions on persistence bar codes. Homology, Homotopy and Applications , 18:381-402, 2016 H. Adams, S. Chepushtanova, T. Emerson, E. Hanson, M. Kirby, F. Motta, R. Neville, C. Peterson, P. Shipman, and L. Ziegelmeier. Persistence images: A stable vector representation of persistent homology. Journal of Machine Learning Research , 18:1-35, 2017. P. Bendich, J.S. Marron, E. Miller, A. Pieloch, and S. Skwerer, Persistent homology analysis of brain artery trees. Ann. Appl. Stat. , 10:198-218, 2016. M. A. Armstrong. Basic Topology . Springer, New York, Berlin, 1983. N. Atienza, R. Gonzalez-Diaz, and M. Soriano-Trigueros. On the stability of persistent entropy and new summary functions for topological data analysis. arXiv:1803.08304, 2018. U. Bauer. Ripser: A lean C++ code for the computation of Vietoris-Rips persistence barcodes. Software available at https://github.com/Ripser/ripser , 2015. M. W. Brown, S. Martin, S. N. Pollock, E. A. Coutsias, and J. P. Watson. Algorithmic dimensionality reduction for molecular structure analysis. Journal of Chemical Physics , 129:064118, 2008. P. Bubenik. Statistical topological data analysis using persistence landscapes. Journal of Machine Learning Research , 16:77-10, 2015. P. Bubenik and P. Dlotko, A persistence lanscapes toolbox for topological statistics. Journal of Symbolic Computation , 78:91-114, 2017. G. Carlson, T. Ishkhanov, V. de Silva, and A. Zomorodian. On the local behavior of spaces of natural images. Int. J. Computer Vision , 76:1-12, 2008. M. Carriere, U. Bauer. On the metric distortion of embedding persistence diagrams into reproducing kernel Hilbert spaces. arXiv:1806.06924, 2018 M. Carriere, M. Cuturi, and S. Oudot. Sliced Wasserstein kernel for persistence diagrams. arXiv:1706.03358, 2017. M. Carriere, S. Oudot, and M. Ovsjanikov. Stable topological signatures for points on 3d shapes. In Computer Graphics Forum , 34:1-2, 2015. Y.-C. Chen, D. Wong, A. Rinaldo, and L. Wasserman. Statistical analysis of persistence intensity functions. arXiv:1510.02502, 2015. L. Crawford, A. Monod, A. X. Chen, S. Mukherjee, and R. Rabadan. Functional data analysis using a topological summary statistic: The smooth Euler characteristic transform. arXiv: https://arxiv.org/abs/1611.06818 , 2017. F. Chazal, V. de Silva, and S. Oudot. Persistence stability for geometric complexes. Geometriae Dedicata , pages 1-22, 2013. F. Chazal and V. Divol. The density of expected persistence diagrams and its kernel based estimation. arXiv:1802.10457, 2018. I. Chevyrev, V. Nanda, H. Oberhauser. Persistence paths and signature features in topological data analysis. arXiv:1806.00381, 2018. M.K. Chung, P. Bubenik, and P.T. Kim. Persistence diagrams of cortical surface data. In Information Processing in Medical Imaging , 386-297, Springer, 2009. B. Di Fabio and M. Ferri. Comparing persistence diagrams through complex vectors. In International Conference on Image Analysis and Processing , LNCS 9279, pages 294-305, 2015. H. Edelsbrunner and J. Harer. Computational Topology: An Introduction . American Mathematical Society, Providence, 2010. H. Edelsbrunner, D. Letscher, and A. Zomorodian. Topological persistence and simplification. Discrete Comput. Geom. , 28:511-533, 2002. A. Hatcher. Algebraic Topology . Cambridge University Press, Cambridge, 2002. C. Hofer, R. Kwit, M. Niethammer, and A. Uhl. Deep learning with topological signatures. In Advances in Neural Information Processing Systems , pages 1634-1644, 2017. S. Kalisnik. Tropical coordinates on the space of persistence barcodes. In Foundations of Computational Mathematics , pages 1-29, 2018. A. B. Lee, K. S. Pedersen, and D. Mumford. The nonlinear statistics of high-contrast patches in natural images. Int. J. Comput. Vision , 54:83-103, 2003. S. Martin and J. P. Watson. Non-manifold surface reconstruction from high-dimensional point cloud data. Computational Geometry , 44:427-441, 2011. S. Martin, A. Thompson, E. A. Coutsias, and J. P. Watson. Topology of cyclo-octane energy landscape. Journal of Chemical Physics , 132: 234115, 2010. J. Reininghaus, S. Huber, U. Bauer, and R. Kwitt. A stable multi-scale kernel for topological machine learning. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition , pages 4741-4748, 2015. P. Skraba. Persistent homology and machine learning. Informatica , 24(2), 2018. C. Topaz, L. Ziegelmeier, and T. Halverson. Topological data analysis of biological aggregation models. PloS One, 10(5):e0126383, 2015. M. Zeppelzauer, B. Zielinski, M. Juda, and M. Seidl. Topological descriptors for 3d surface analysis. In Computational Topology in Image Context: 6 th International Workshop Proceedings , pages 77-87, 2016. J. H. van Hateren and A. van der Schaaf. Independent component filters of natural images compared with simple cells in primary visual cortex. Proc. R. Soc. Lond. B , 265:359-366, 1998. A. Zomorodian. Advances in Applied and Computational Topology . American Mathematical Society, 2012. A. Zomorodian and G. Carlsson. Computing persistent homology. Discrete Comput. Geom ., 33:249-274, 2005. (See also Part 1 , Part 2 , and Part 3 .)","title":"Persistent Homology Tutorial 4 of 4"},{"location":"persistent-homology-tutorial-4/#persistent-homology-tutorial-4-of-4","text":"(See also Part 1 , Part 2 , and Part 3 .)","title":"Persistent Homology Tutorial 4 of 4"},{"location":"persistent-homology-tutorial-4/#topological-feature-vectors-persistent-homology-and-machine-learning","text":"There are by now a wide variety of ways to incorporate persistent homology information into feature vectors for a machine learning task (Adcock et al 2016, Adams et al 2017, Atienza 2018, Bendich et al 2016, Bubenik 2015, Bubenik & Dlotko 2016, Carriere et al 2015, Carriere & Bauer 2018, Carriere et al 2018, Chazal & Divol 2018, Chen et al 2015, Chevyrev et al 2018, Chung et al 2009, Di Fabio & Ferri 2015, Hofer et al 2017, Kalisnik 2018, Reininghaus et al 2015, Skraba 2018, Topaz et al 2015, Zeppelzauer et al 2016). We tersely describe a few of these approaches. For Henry's \"from persistent homology to machine learning\" survey slides from the ICERM TRIPODS bootcamp, please see here .","title":"Topological feature vectors - persistent homology and machine learning"},{"location":"persistent-homology-tutorial-4/#persistence-landscpaes","text":"One example is persistence landscapes (Bubenik 2015). Bubenik & Dlotko (2017) have made their code available , and persistence landscapes are also implementable in R-TDA .","title":"Persistence landscpaes"},{"location":"persistent-homology-tutorial-4/#persistence-images","text":"A second example is persistence images (Adams et al 2017), with Python code available from Nathaniel Saul (Persims) or Francis Motta , and with Matlab code available here . We'll attempt to describe how to compute persistence images using the Jupyter notebooks accompanying Nathaniel Saul's Persim code . First, download the Persim code. Second, install Jupyter , perhaps using Anaconda as described in that link. Open a new terminal (which is often required after new installations) and in that terminal, try the following command. jupyter notebook This should open a window in an internet browser. In this browser window, change directories to persim/notebooks , and then open the Jupyter notebook Persistence Images.ipynb . You should then be able to run the code in the browser!","title":"Persistence images"},{"location":"persistent-homology-tutorial-4/#topological-feature-vectors-coding-challenges","text":"","title":"Topological feature vectors - coding challenges"},{"location":"persistent-homology-tutorial-4/#6-shape-classes","text":"Change directory to the folder topological-data-analysis/topological-feature-vectors/data-6-shape-classes . NOTE: Folder missing--must fix! This subfolder contains persistence diagram data for 6 shape classes: A unit cube A circle of diameter one A sphere of diameter one Three clusters with centers randomly chosen in the unit cube Three clusters within three clusters A torus with a major diameter of one and a minor diameter of one half. These shape classes are described in Section 6.1 of the paper Persistence Images: A Stable Vector Representation of Persistent Homology , and are shown below. We produced 25 point clouds of 500 randomly sampled points from each shape class. We then added a level of Gaussian noise to each point, at a noise level neta=0.1 or neta=0.05. We then have already computed the persistent homology intervals in homological dimension i=0 and i=1. Should \"neta\" here be \"eta\" (Greek letter)? If so, we can then use MathJax on these equations. For example, the file ToyData_PD_n05_23_6_0.txt corresponds to noise level neta=0.05, the 23 rd point cloud randomly sampled from shape class 6., with persistent homology computed in dimension 0. Each row of this file has two entries: the birth and death time of a 0-dimensional persistent homology interval. By contrast, the file ToyData_PD_n1_21_3_1.txt corresponds to noise level neta=1, the 21 st point cloud randomly sampled from shape class 3., with persistent homology computed in dimension 1. Each row of this file has two entries: the birth and death time of a 1-dimensional persistent homology interval. Your task is to use machine learning to distinguish these six classes from each other. In a K-medoids clustering test, some accuracies and computation times are displayed for bottleneck distances, Wasserstein distances, persistence landscapes, and persistence images in Table 1 of Persistence Images: A Stable Vector Representation of Persistent Homology . Do you have ideas for beating these accuracies or computation times? In the above table, PD means the persistence diagram equipped with either the L^1 L^1 (1-Wasserstein), L^2 L^2 (2-Wasserstein), or sup (bottleneck) metrics. PL means the persistence landscape, equipped with either the L^1 L^1 , L^2 L^2 , or sup metric. PI means the persistence image, equipped with either the L^1 L^1 , L^2 L^2 , or sup metric. But you should develop your own techniques for turning barcodes into feature vectors, and see how they compare! Some Matlab code for creating points from these shape classes is available here .","title":"6 shape classes"},{"location":"persistent-homology-tutorial-4/#bibliography","text":"A. Adcock, E. Carlsson, and G. Carlsson. The ring of algebraic functions on persistence bar codes. Homology, Homotopy and Applications , 18:381-402, 2016 H. Adams, S. Chepushtanova, T. Emerson, E. Hanson, M. Kirby, F. Motta, R. Neville, C. Peterson, P. Shipman, and L. Ziegelmeier. Persistence images: A stable vector representation of persistent homology. Journal of Machine Learning Research , 18:1-35, 2017. P. Bendich, J.S. Marron, E. Miller, A. Pieloch, and S. Skwerer, Persistent homology analysis of brain artery trees. Ann. Appl. Stat. , 10:198-218, 2016. M. A. Armstrong. Basic Topology . Springer, New York, Berlin, 1983. N. Atienza, R. Gonzalez-Diaz, and M. Soriano-Trigueros. On the stability of persistent entropy and new summary functions for topological data analysis. arXiv:1803.08304, 2018. U. Bauer. Ripser: A lean C++ code for the computation of Vietoris-Rips persistence barcodes. Software available at https://github.com/Ripser/ripser , 2015. M. W. Brown, S. Martin, S. N. Pollock, E. A. Coutsias, and J. P. Watson. Algorithmic dimensionality reduction for molecular structure analysis. Journal of Chemical Physics , 129:064118, 2008. P. Bubenik. Statistical topological data analysis using persistence landscapes. Journal of Machine Learning Research , 16:77-10, 2015. P. Bubenik and P. Dlotko, A persistence lanscapes toolbox for topological statistics. Journal of Symbolic Computation , 78:91-114, 2017. G. Carlson, T. Ishkhanov, V. de Silva, and A. Zomorodian. On the local behavior of spaces of natural images. Int. J. Computer Vision , 76:1-12, 2008. M. Carriere, U. Bauer. On the metric distortion of embedding persistence diagrams into reproducing kernel Hilbert spaces. arXiv:1806.06924, 2018 M. Carriere, M. Cuturi, and S. Oudot. Sliced Wasserstein kernel for persistence diagrams. arXiv:1706.03358, 2017. M. Carriere, S. Oudot, and M. Ovsjanikov. Stable topological signatures for points on 3d shapes. In Computer Graphics Forum , 34:1-2, 2015. Y.-C. Chen, D. Wong, A. Rinaldo, and L. Wasserman. Statistical analysis of persistence intensity functions. arXiv:1510.02502, 2015. L. Crawford, A. Monod, A. X. Chen, S. Mukherjee, and R. Rabadan. Functional data analysis using a topological summary statistic: The smooth Euler characteristic transform. arXiv: https://arxiv.org/abs/1611.06818 , 2017. F. Chazal, V. de Silva, and S. Oudot. Persistence stability for geometric complexes. Geometriae Dedicata , pages 1-22, 2013. F. Chazal and V. Divol. The density of expected persistence diagrams and its kernel based estimation. arXiv:1802.10457, 2018. I. Chevyrev, V. Nanda, H. Oberhauser. Persistence paths and signature features in topological data analysis. arXiv:1806.00381, 2018. M.K. Chung, P. Bubenik, and P.T. Kim. Persistence diagrams of cortical surface data. In Information Processing in Medical Imaging , 386-297, Springer, 2009. B. Di Fabio and M. Ferri. Comparing persistence diagrams through complex vectors. In International Conference on Image Analysis and Processing , LNCS 9279, pages 294-305, 2015. H. Edelsbrunner and J. Harer. Computational Topology: An Introduction . American Mathematical Society, Providence, 2010. H. Edelsbrunner, D. Letscher, and A. Zomorodian. Topological persistence and simplification. Discrete Comput. Geom. , 28:511-533, 2002. A. Hatcher. Algebraic Topology . Cambridge University Press, Cambridge, 2002. C. Hofer, R. Kwit, M. Niethammer, and A. Uhl. Deep learning with topological signatures. In Advances in Neural Information Processing Systems , pages 1634-1644, 2017. S. Kalisnik. Tropical coordinates on the space of persistence barcodes. In Foundations of Computational Mathematics , pages 1-29, 2018. A. B. Lee, K. S. Pedersen, and D. Mumford. The nonlinear statistics of high-contrast patches in natural images. Int. J. Comput. Vision , 54:83-103, 2003. S. Martin and J. P. Watson. Non-manifold surface reconstruction from high-dimensional point cloud data. Computational Geometry , 44:427-441, 2011. S. Martin, A. Thompson, E. A. Coutsias, and J. P. Watson. Topology of cyclo-octane energy landscape. Journal of Chemical Physics , 132: 234115, 2010. J. Reininghaus, S. Huber, U. Bauer, and R. Kwitt. A stable multi-scale kernel for topological machine learning. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition , pages 4741-4748, 2015. P. Skraba. Persistent homology and machine learning. Informatica , 24(2), 2018. C. Topaz, L. Ziegelmeier, and T. Halverson. Topological data analysis of biological aggregation models. PloS One, 10(5):e0126383, 2015. M. Zeppelzauer, B. Zielinski, M. Juda, and M. Seidl. Topological descriptors for 3d surface analysis. In Computational Topology in Image Context: 6 th International Workshop Proceedings , pages 77-87, 2016. J. H. van Hateren and A. van der Schaaf. Independent component filters of natural images compared with simple cells in primary visual cortex. Proc. R. Soc. Lond. B , 265:359-366, 1998. A. Zomorodian. Advances in Applied and Computational Topology . American Mathematical Society, 2012. A. Zomorodian and G. Carlsson. Computing persistent homology. Discrete Comput. Geom ., 33:249-274, 2005. (See also Part 1 , Part 2 , and Part 3 .)","title":"Bibliography"}]}